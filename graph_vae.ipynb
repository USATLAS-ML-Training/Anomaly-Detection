{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    \n",
    "    ''' basic graph convolution layer computing X' = act(AXW) ''' \n",
    "    \n",
    "    def __init__(self, output_sz, activation, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.output_sz = output_sz\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.output_sz], initializer=tf.keras.initializers.GlorotUniform())\n",
    "\n",
    "    def call(self, inputs, adjacency):\n",
    "        x = tf.matmul(inputs, self.kernel)\n",
    "        x = tf.matmul(adjacency, x)\n",
    "        return self.activation(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(GraphConvolution, self).get_config()\n",
    "        config.update({'output_sz': self.output_sz, 'activation': self.activation})\n",
    "        return config\n",
    "\n",
    "\n",
    "class InnerProductDecoder(tf.keras.layers.Layer):\n",
    "\n",
    "    ''' inner product decoder reconstructing adjacency matrix as act(z^T z) \n",
    "        input assumed of shape [batch_sz x n_nodes x z_d]\n",
    "        where \n",
    "            batch_sz can be 1 for single example feeding\n",
    "            n_nodes ... number of nodes in graph\n",
    "            z_d ... dimensionality of latent space\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_t = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "        adjacency_hat = tf.matmul(inputs, z_t)\n",
    "        return self.activation(adjacency_hat)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(InnerProductDecoder, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def adjacency_loss_from_logits(adj_orig, adj_pred, pos_weight):\n",
    "    # cast probability to a_ij = 1 if > 0.5 or a_ij = 0 if <= 0.5\n",
    "    return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=adj_pred, labels=adj_orig, pos_weight=pos_weight)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAutoencoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, nodes_n, feat_sz, activation, **kwargs):\n",
    "        super(GraphAutoencoder, self).__init__(**kwargs)\n",
    "        self.nodes_n = nodes_n\n",
    "        self.feat_sz = feat_sz\n",
    "        self.input_shape_feat = [self.nodes_n, self.feat_sz]\n",
    "        self.input_shape_adj = [self.nodes_n, self.nodes_n]\n",
    "        self.activation = activation\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = lays.InnerProductDecoder(activation=tf.keras.activations.linear) # if activation sigmoid -> return probabilities from logits\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        ''' reduce feat_sz to 1 '''\n",
    "        inputs_feat = tf.keras.layers.Input(shape=self.input_shape_feat, dtype=tf.float32, name='encoder_input_features')\n",
    "        inputs_adj = tf.keras.layers.Input(shape=self.input_shape_adj, dtype=tf.float32, name='encoder_input_adjacency')\n",
    "        x = inputs_feat\n",
    "        #feat_sz-1 layers needed to reduce to R^1 \n",
    "        for output_sz in reversed(range(2, self.feat_sz)):\n",
    "            x = lays.GraphConvolution(output_sz=output_sz, activation=self.activation)(x, inputs_adj)\n",
    "        # NO activation before latent space: last graph with linear pass through activation\n",
    "        x = lays.GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "        encoder = tf.keras.Model(inputs=(inputs_feat, inputs_adj), outputs=x)\n",
    "        encoder.summary()\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self.encoder(inputs)\n",
    "        adj_reco = self.decoder(z)\n",
    "        return adj_reco\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.math.reduce_sum(adj_orig)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            adj_pred = self((X, adj_tilde))  # Forward pass\n",
    "            # Compute the loss value\n",
    "            loss = adjacency_loss_from_logits(adj_orig, adj_pred, pos_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.math.reduce_sum(adj_orig)\n",
    "\n",
    "        adj_pred = self((X, adj_tilde), training=False)\n",
    "        # loss = adjacency_loss(adj_orig, adj_pred) # TODO: add regularization\n",
    "        loss = adjacency_loss_from_logits(adj_orig, adj_pred, pos_weight) # TODO: add regularization\n",
    "        return {'loss' : loss}\n",
    "\n",
    "\n",
    "\n",
    "class GraphVariationalAutoencoder(GraphAutoencoder):\n",
    "\n",
    "    def build_encoder(self):\n",
    "\n",
    "        ''' reduce feat_sz to 2 '''\n",
    "        inputs_feat = tf.keras.layers.Input(shape=self.input_shape_feat, dtype=tf.float32, name='encoder_input_features')\n",
    "        inputs_adj = tf.keras.layers.Input(shape=self.input_shape_adj, dtype=tf.float32, name='encoder_input_adjacency')\n",
    "        x = inputs_feat\n",
    "\n",
    "        for output_sz in reversed(range(2, self.feat_sz)):\n",
    "            x = lays.GraphConvolution(output_sz=output_sz, activation=self.activation)(x, inputs_adj)\n",
    "\n",
    "        ''' make latent space params mu and sigma in last compression to feat_sz = 1 '''\n",
    "        self.z_mean = lays.GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "        self.z_log_var = lays.GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "\n",
    "        self.z = self.z_mean + tf.random_normal(self.nodes_n) * tf.exp(self.z_log_std)\n",
    "\n",
    "        return tf.keras.Model(inputs=(inputs_feat, inputs_adj), outputs=self.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
