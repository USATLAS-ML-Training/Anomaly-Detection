{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Models\n",
    "\n",
    "***\n",
    "2 model types:\n",
    "- graph autoencoder\n",
    "- graph variational autoencoder\n",
    "\n",
    "4 steps:\n",
    "- model class definitions\n",
    "- input data loading and processing\n",
    "- training\n",
    "- inference\n",
    "\n",
    "***\n",
    "\n",
    "layers adapted from [Kipf et al.](https://arxiv.org/abs/1609.02907)  \n",
    "author: Kinga Anna Wozniak  \n",
    "\n",
    "note: graph variational autoenoder takes input as multiple of batch size (train and validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(particles):\n",
    "    idx_pt, idx_eta, idx_phi, idx_class = range(4)\n",
    "    # min-max normalize pt\n",
    "    particles[:,:,idx_pt] = (particles[:,:,idx_pt] - np.min(particles[:,:,idx_pt])) / (np.max(particles[:,:,idx_pt])-np.min(particles[:,:,idx_pt]))\n",
    "    # standard normalize angles\n",
    "    particles[:,:,idx_eta] = (particles[:,:,idx_eta] - np.mean(particles[:,:,idx_eta]))/np.std(particles[:,:,idx_eta])\n",
    "    particles[:,:,idx_eta] = (particles[:,:,idx_eta] - np.mean(particles[:,:,idx_phi]))/np.std(particles[:,:,idx_phi])\n",
    "    # min-max normalize class label\n",
    "    particles[:,:,idx_class] = (particles[:,:,idx_class] - np.min(particles[:,:,idx_class])) / (np.max(particles[:,:,idx_class])-np.min(particles[:,:,idx_class]))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adjacency(A):\n",
    "    D = np.array(np.sum(A, axis=2), dtype=np.float32) # compute outdegree (= rowsum)\n",
    "    D = np.nan_to_num(np.power(D,-0.5), posinf=0, neginf=0) # normalize (**-(1/2))\n",
    "    D = np.asarray([np.diagflat(dd) for dd in D]) # and diagonalize\n",
    "    return np.matmul(D, np.matmul(A, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adjacencies(particles):\n",
    "    real_p_mask = particles[:,:,0] > 0 # construct mask for real particles\n",
    "    adjacencies = (real_p_mask[:,:,np.newaxis] * real_p_mask[:,np.newaxis,:]).astype('float32')\n",
    "    return adjacencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Latent Space Loss (KL-Divergence)\n",
    "@tf.function\n",
    "def kl_loss(z_mean, z_log_var):\n",
    "    kl = 1. + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    return -0.5 * tf.reduce_mean(kl, axis=-1) # multiplying mse by N -> using sum (instead of mean) in kl loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GC layers adapted from Kipf: https://github.com/tkipf/gae/blob/0ebbe9b9a8f496eb12deb9aa6a62e7016b5a5ac3/gae/layers.py '''\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    \n",
    "    ''' basic graph convolution layer performing act(AXW) '''\n",
    "    \n",
    "    def __init__(self, output_sz, activation=tf.keras.activations.linear, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.output_sz = output_sz\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.output_sz], initializer=tf.keras.initializers.GlorotUniform())\n",
    "\n",
    "\n",
    "    def call(self, inputs, adjacency):\n",
    "        x = tf.matmul(inputs, self.kernel)\n",
    "        x = tf.matmul(adjacency, x)\n",
    "        return self.activation(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(GraphConvolution, self).get_config()\n",
    "        config.update({'output_sz': self.output_sz, 'activation': self.activation})\n",
    "        return config\n",
    "\n",
    "\n",
    "class InnerProductDecoder(tf.keras.layers.Layer):\n",
    "\n",
    "    ''' inner product decoder reconstructing adjacency matrix as act(z^T z) \n",
    "        input assumed of shape [batch_sz x n_nodes x z_d]\n",
    "        where \n",
    "            batch_sz can be 1 for single example feeding\n",
    "            n_nodes ... number of nodes in graph\n",
    "            z_d ... dimensionality of latent space\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation=tf.keras.activations.linear, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        perm = [0, 2, 1] if len(inputs.shape) == 3 else [1, 0]\n",
    "        z_t = tf.transpose(inputs, perm=perm)\n",
    "        adjacency_hat = tf.matmul(inputs, z_t)\n",
    "        return self.activation(adjacency_hat)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(InnerProductDecoder, self).get_config()\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAutoencoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, nodes_n, feat_sz, activation=tf.nn.tanh, **kwargs):\n",
    "        super(GraphAutoencoder, self).__init__(**kwargs)\n",
    "        self.nodes_n = nodes_n\n",
    "        self.feat_sz = feat_sz\n",
    "        self.input_shape_feat = [self.nodes_n, self.feat_sz]\n",
    "        self.input_shape_adj = [self.nodes_n, self.nodes_n]\n",
    "        self.activation = activation\n",
    "        self.loss_fn = tf.nn.weighted_cross_entropy_with_logits\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = InnerProductDecoder(activation=tf.keras.activations.linear) # if activation sigmoid -> return probabilities from logits\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        ''' reduce feat_sz to 2 '''\n",
    "        inputs_feat = tf.keras.layers.Input(shape=self.input_shape_feat, dtype=tf.float32, name='encoder_input_features')\n",
    "        inputs_adj = tf.keras.layers.Input(shape=self.input_shape_adj, dtype=tf.float32, name='encoder_input_adjacency')\n",
    "        x = inputs_feat\n",
    "        #feat_sz-1 layers needed to reduce to R^2 \n",
    "        for output_sz in reversed(range(2, self.feat_sz)):\n",
    "            x = GraphConvolution(output_sz=output_sz, activation=self.activation)(x, inputs_adj)\n",
    "        x = GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "        encoder = tf.keras.Model(inputs=(inputs_feat, inputs_adj), outputs=x)\n",
    "        encoder.summary()\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self.encoder(inputs)\n",
    "        adj_pred = self.decoder(z)\n",
    "        return z, adj_pred\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        # pos_weight = zero-adj / one-adj -> no-edge vs edge ratio\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.cast(tf.math.reduce_sum(adj_orig), tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z, adj_pred = self((X, adj_tilde))  # Forward pass\n",
    "            # Compute the loss value (binary cross entropy for a_ij in {0,1})\n",
    "            loss = self.loss_fn(labels=adj_orig, logits=adj_pred, pos_weight=pos_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.cast(tf.math.reduce_sum(adj_orig), tf.float32)\n",
    "\n",
    "        z, adj_pred = self((X, adj_tilde), training=False)  # Forward pass\n",
    "        loss = tf.math.reduce_mean(self.loss_fn(labels=adj_orig, logits=adj_pred, pos_weight=pos_weight)) # TODO: add regularization\n",
    "        \n",
    "        return {'loss' : loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variational graph autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphVariationalAutoencoder(GraphAutoencoder):\n",
    "    \n",
    "    def __init__(self, nodes_n, feat_sz, activation, **kwargs):\n",
    "        super(GraphVariationalAutoencoder, self).__init__(nodes_n, feat_sz, activation, **kwargs)\n",
    "        self.loss_fn_latent = kl_loss\n",
    "\n",
    "    def build_encoder(self):\n",
    "\n",
    "        ''' reduce feat_sz to 2 '''\n",
    "        inputs_feat = tf.keras.layers.Input(shape=self.input_shape_feat, dtype=tf.float32, name='encoder_input_features')\n",
    "        inputs_adj = tf.keras.layers.Input(shape=self.input_shape_adj, dtype=tf.float32, name='encoder_input_adjacency')\n",
    "        x = inputs_feat\n",
    "\n",
    "        for output_sz in reversed(range(2, self.feat_sz)):\n",
    "            x = GraphConvolution(output_sz=output_sz, activation=self.activation)(x, inputs_adj)\n",
    "\n",
    "        ''' make latent space params mu and sigma in last compression to feat_sz = 1 '''\n",
    "        self.z_mean = GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "        self.z_log_var = GraphConvolution(output_sz=1, activation=tf.keras.activations.linear)(x, inputs_adj)\n",
    "\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(self.z_mean)[0], self.nodes_n, 1))\n",
    "        self.z = self.z_mean +  epsilon * tf.exp(0.5 * self.z_log_var)\n",
    "\n",
    "        return tf.keras.Model(inputs=(inputs_feat, inputs_adj), outputs=[self.z, self.z_mean, self.z_log_var])\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z, z_mean, z_log_var = self.encoder(inputs)\n",
    "        adj_pred = self.decoder(z)\n",
    "        return z, z_mean, z_log_var, adj_pred\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.cast(tf.math.reduce_sum(adj_orig), tf.float32)\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z, z_mean, z_log_var, adj_pred = self((X, adj_tilde))  # Forward pass\n",
    "            # Compute the loss value (binary cross entropy for a_ij in {0,1})\n",
    "            loss_reco = tf.math.reduce_mean(self.loss_fn(labels=adj_orig, logits=adj_pred, pos_weight=pos_weight), axis=(1,2)) # TODO: add regularization\n",
    "            loss_latent = tf.math.reduce_mean(self.loss_fn_latent(z_mean, z_log_var), axis=1)\n",
    "            loss = loss_reco + loss_latent\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {'loss' : loss_reco+loss_latent, 'loss_reco': loss_reco, 'loss_latent': loss_latent}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (X, adj_tilde), adj_orig = data\n",
    "        pos_weight = tf.cast(adj_orig.shape[1] * adj_orig.shape[2] - tf.math.reduce_sum(adj_orig), tf.float32) / tf.cast(tf.math.reduce_sum(adj_orig), tf.float32)\n",
    "\n",
    "        z, z_mean, z_log_var, adj_pred = self((X, adj_tilde))  # Forward pass\n",
    "        # Compute the loss value (binary cross entropy for a_ij in {0,1})\n",
    "        loss_reco =  tf.math.reduce_mean(self.loss_fn(labels=adj_orig, logits=adj_pred, pos_weight=pos_weight)) # TODO: add regularization\n",
    "        loss_latent = tf.math.reduce_mean(self.loss_fn_latent(z_mean, z_log_var))\n",
    "        \n",
    "        return {'loss' : loss_reco+loss_latent, 'loss_reco': loss_reco, 'loss_latent': loss_latent}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the correct background filename\n",
    "filename = 'background.h5'\n",
    "ff = h5py.File(filename, 'r')\n",
    "particles = np.asarray(ff.get('Particles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 19, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_n = particles.shape[1]\n",
    "feat_sz = particles.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: array([b'Pt', b'Eta', b'Phi', b'Class'], dtype='|S5')\n",
    "batch_size = 128\n",
    "particles_train = particles[:batch_size*20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "A = make_adjacencies(particles_train)\n",
    "A_tilde = normalized_adjacency(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'pt')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOvUlEQVR4nO3dXYxc513H8e+viVJEaAM0RqJ+6aZyGnVVLlqt0iIkCGpBdoPjqq2KLSooMrEScG+4wVW4QLxI6QVcRBiCpQbzVqcmQpWjOEoVSGSpSkpcGtq8EOQYB29a1U7TWmp5SUP/XMxQJhuv96xnZk/m2e9HsjRz5uyc/+NZ/3z2f549T6oKSVJbXtd3AZKkyTPcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd2kFSW5Isth3HdJqGO6S1CDDXRpKcjrJJ5I8leSbSf48yZXA/cCbk3x7+OfNfdcqrSTefkAaSHIa+DawHfgOcC/wEPAg8NdVtam/6qTV8cxdeqU/rqozVfUi8AfA7r4Lki6F4S690pmRx88BtmA0kwx36ZU2jzzeAnwVsHepmWO4S6/0G0k2JflR4DbgM8DXgTcluarf0qTuDHfplT4NfA44BTwL/H5V/QtwGDiV5FvOltEscLaMNDScLfNrVfVg37VI4/LMXZIaZLhLUoNsy0hSgzxzl6QGXd53AQBXX311zc3N9V2GJM2UL37xiy9U1YYLvfaaCPe5uTlOnDjRdxmSNFOSPLfca722ZZLsSHLw/PnzfZYhSc3pNdyr6t6q2nvVVf7inyRNkhdUJalBtmUkqUG2ZSSpQbZlJKlBtmUkqUG2ZSSpQa+JX2Iax9z++3o79unbb+zt2JJ0MfbcJalB9twlqUH23CWpQbZlJKlBhrskNchwl6QGGe6S1CBny0hSg5wtI0kNsi0jSQ0y3CWpQYa7JDXIcJekBhnuktQgp0JKUoOcCilJDbItI0kNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQVMI9yZVJTiT5hWm8vyTp4jqFe5K7kpxN8sSS7duSPJPkZJL9Iy/9FnBkkoVKkrrreuZ+CNg2uiHJZcABYDswD+xOMp/k54CngLMTrFOStAqXd9mpqo4nmVuy+XrgZFWdAkhyN7AT+CHgSgaB/59JjlXV95a+Z5K9wF6ALVu2XPIAJEmv1incl7ERODPyfBF4d1XtA0jyMeCFCwU7QFUdBA4CLCws1Bh1SJKWGCfcL6qqDq20T5IdwI6tW7dOqwxJWpfGmS3zPLB55Pmm4bbOvOWvJE3HOOH+GHBtkmuSXAHsAo6u5g1crEOSpqPrVMjDwCPAdUkWk+ypqpeBfcADwNPAkap6cjUH98xdkqaj62yZ3ctsPwYcu9SD23OXpOlwmT1JapD3lpGkBvUa7l5QlaTpsC0jSQ2yLSNJDTLcJalB9twlqUH23CWpQbZlJKlBhrskNcieuyQ1yJ67JDVoaot1rAdz++/r5binb7+xl+NKmh323CWpQYa7JDXIC6qS1CAvqEpSg2zLSFKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAY5z12SGuQ8d0lqkG0ZSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNPFwT/L2JHcmuSfJrZN+f0nSyjqFe5K7kpxN8sSS7duSPJPkZJL9AFX1dFXdAnwE+KnJlyxJWknXM/dDwLbRDUkuAw4A24F5YHeS+eFrNwH3AccmVqkkqbNO4V5Vx4EXl2y+HjhZVaeq6iXgbmDncP+jVbUd+KXl3jPJ3iQnkpw4d+7cpVUvSbqgy8f42o3AmZHni8C7k9wAfBB4PRc5c6+qg8BBgIWFhRqjDknSEuOE+wVV1cPAw132TbID2LF169ZJlyFJ69o4s2WeBzaPPN803NaZd4WUpOkYJ9wfA65Nck2SK4BdwNHVvIH3c5ek6eg6FfIw8AhwXZLFJHuq6mVgH/AA8DRwpKqeXM3BPXOXpOno1HOvqt3LbD+G0x0l6TXHZfYkqUEusydJDfLGYZLUINsyktQg2zKS1CDbMpLUoInffmA1vP3ApZnbf19vxz59+429HVtSd7ZlJKlBtmUkqUGGuyQ1yKmQktQge+6S1CDbMpLUIMNdkhpkuEtSgwx3SWqQs2UkqUHOlpGkBtmWkaQGGe6S1CDDXZIaZLhLUoMMd0lqkFMhJalBToWUpAbZlpGkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ16PJpvGmSDwA3Am8EPlVVn5vGcSRJF9b5zD3JXUnOJnliyfZtSZ5JcjLJfoCq+mxV3QzcAvziZEuWJK1kNW2ZQ8C20Q1JLgMOANuBeWB3kvmRXX57+LokaQ11DveqOg68uGTz9cDJqjpVVS8BdwM7M/BJ4P6q+qcLvV+SvUlOJDlx7ty5S61fknQB415Q3QicGXm+ONz2ceB9wIeT3HKhL6yqg1W1UFULGzZsGLMMSdKoqVxQrao7gDtW2i/JDmDH1q1bp1GGJK1b4565Pw9sHnm+abitE2/5K0nTMW64PwZcm+SaJFcAu4CjXb/YxTokaTpWMxXyMPAIcF2SxSR7quplYB/wAPA0cKSqnuz6np65S9J0dO65V9XuZbYfA45dysHtuc+euf339XLc07ff2MtxpVnlMnuS1CDvLSNJDeo13L2gKknTYVtGkhpkW0aSGmS4S1KD7LlLUoPsuUtSg2zLSFKDDHdJapA9d0lqkD13SWqQbRlJapDhLkkNMtwlqUFeUJWkBnlBVZIaZFtGkhrUeZk9qU8u7yetjmfuktQgw12SGmS4S1KDDHdJapDz3CWpQc5zl6QG2ZaRpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBEw/3JG9N8qkk90z6vSVJ3XQK9yR3JTmb5Ikl27cleSbJyST7AarqVFXtmUaxkqRuup65HwK2jW5IchlwANgOzAO7k8xPtDpJ0iXpFO5VdRx4ccnm64GTwzP1l4C7gZ1dD5xkb5ITSU6cO3euc8GSpJWN03PfCJwZeb4IbEzypiR3Au9M8onlvriqDlbVQlUtbNiwYYwyJElLTXyZvar6BnBLl32T7AB2bN26ddJlSBPR1/J+4BJ/Gs84Z+7PA5tHnm8abuvMu0JK0nSME+6PAdcmuSbJFcAu4OhkypIkjaPrVMjDwCPAdUkWk+ypqpeBfcADwNPAkap6cjUHd7EOSZqOTj33qtq9zPZjwLFLPXhV3Qvcu7CwcPOlvock6dVcZk+SGuQye5LUIG8cJkkNsi0jSQ2yLSNJDbItI0kNsi0jSQ2yLSNJDbItI0kNMtwlqUETv+XvanjLX2l5fd1u2FsNt8GeuyQ1yLaMJDXIcJekBhnuktQgw12SGuRvqEpSg5wtI0kNsi0jSQ0y3CWpQYa7JDXIcJekBhnuktQgbxwmad3r6yZtML0btTkVUpIaZFtGkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KCJ/4ZqkiuBPwFeAh6uqr+Z9DEkSRfX6cw9yV1JziZ5Ysn2bUmeSXIyyf7h5g8C91TVzcBNE65XktRB17bMIWDb6IYklwEHgO3APLA7yTywCTgz3O1/JlOmJGk1OrVlqup4krklm68HTlbVKYAkdwM7gUUGAf84F/nPI8leYC/Ali1bVlu3pClp8SZa69E4F1Q38v9n6DAI9Y3A3wEfSvKnwL3LfXFVHayqhapa2LBhwxhlSJKWmvgF1ar6DvCrXfb1lr+SNB3jnLk/D2weeb5puK0zb/krSdMxTrg/Blyb5JokVwC7gKOreYMkO5IcPH/+/BhlSJKW6joV8jDwCHBdksUke6rqZWAf8ADwNHCkqp5czcE9c5ek6eg6W2b3MtuPAccmWpEkaWy93n7AtowkTYdrqEpSgzxzl6QGpar6roEk54DnLvHLrwZemGA5s8Axrw+OeX0YZ8xvqaoL/hboayLcx5HkRFUt9F3HWnLM64NjXh+mNWbv5y5JDTLcJalBLYT7wb4L6IFjXh8c8/owlTHPfM9dkvRqLZy5S5KWMNwlqUEzE+7LrNc6+vrrk3xm+PoXLrBy1MzpMObfTPJUki8n+fskb+mjzklaacwj+30oSSWZ+WlzXcac5CPDz/rJJJ9e6xonrcP39pYkDyX50vD7+/191Dkpy61DPfJ6ktwx/Pv4cpJ3jX3QqnrN/wEuA54F3gpcAfwzML9kn18H7hw+3gV8pu+612DMPwv84PDxrethzMP93gAcBx4FFvquew0+52uBLwE/Mnz+Y33XvQZjPgjcOnw8D5zuu+4xx/zTwLuAJ5Z5/f3A/UCA9wBfGPeYs3Lm/v31WqvqJeD/1msdtRP4i+Hje4D3Jska1jhpK465qh6qqv8YPn2UwYIps6zL5wzwe8Angf9ay+KmpMuYbwYOVNU3Aarq7BrXOGldxlzAG4ePrwK+uob1TVxVHQdevMguO4G/rIFHgR9O8uPjHHNWwn259VovuE8N7jV/HnjTmlQ3HV3GPGoPg//5Z9mKYx7+uLq5qvpbxXmyunzObwPeluTzSR5Nsm3NqpuOLmP+HeCjSRYZ3Fb842tTWm9W++99RRNfQ1VrL8lHgQXgZ/quZZqSvA74I+BjPZey1i5n0Jq5gcFPZ8eT/ERVfavPoqZsN3Coqv4wyU8Cf5XkHVX1vb4LmxWzcubeZb3W7++T5HIGP8p9Y02qm45Oa9QmeR9wG3BTVf33GtU2LSuN+Q3AO4CHk5xm0Js8OuMXVbt8zovA0ar6blX9G/CvDMJ+VnUZ8x7gCEBVPQL8AIMbbLVq7DWpl5qVcO+yXutR4FeGjz8M/EMNr1TMqBXHnOSdwJ8xCPZZ78PCCmOuqvNVdXVVzVXVHIPrDDdV1Yl+yp2ILt/bn2Vw1k6Sqxm0aU6tYY2T1mXM/w68FyDJ2xmE+7k1rXJtHQV+eThr5j3A+ar62ljv2PdV5FVcbX4/gzOWZ4Hbhtt+l8E/bhh8+H8LnAT+EXhr3zWvwZgfBL4OPD78c7Tvmqc95iX7PsyMz5bp+DmHQTvqKeArwK6+a16DMc8Dn2cwk+Zx4Of7rnnM8R4GvgZ8l8FPYnuAW4BbRj7jA8O/j69M4vva2w9IUoNmpS0jSVoFw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ16H8B+4VbxYo286MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_train[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_train = normalize_features(particles_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'normalized pt')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_train[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('normalized pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model: Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_features (InputLa [(None, 19, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input_adjacency (InputL [(None, 19, 19)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_7 (GraphConvo (None, 19, 3)        12          encoder_input_features[0][0]     \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_8 (GraphConvo (None, 19, 2)        6           graph_convolution_7[0][0]        \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_9 (GraphConvo (None, 19, 1)        2           graph_convolution_8[0][0]        \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gnn = GraphAutoencoder(nodes_n=nodes_n, feat_sz=feat_sz, activation=tf.nn.tanh)\n",
    "gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6739\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6739\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 9/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 25ms/step - val_loss: 0.6738\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 25ms/step - val_loss: 0.6738\n",
      "Epoch 14/100\n",
      "13/15 [=========================>....] - ETA: 0s\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "15/15 [==============================] - 0s 17ms/step - val_loss: 0.6738\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 21ms/step - val_loss: 0.6738\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 23ms/step - val_loss: 0.6738\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 19/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 24/100\n",
      "13/15 [=========================>....] - ETA: 0s\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 22ms/step - val_loss: 0.6738\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 19ms/step - val_loss: 0.6738\n",
      "Epoch 34/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 20ms/step - val_loss: 0.6738\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 49/100\n",
      "13/15 [=========================>....] - ETA: 0s\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "15/15 [==============================] - 0s 20ms/step - val_loss: 0.6738\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 23ms/step - val_loss: 0.6738\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 54/100\n",
      "13/15 [=========================>....] - ETA: 0s\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "15/15 [==============================] - 0s 23ms/step - val_loss: 0.6738\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 22ms/step - val_loss: 0.6738\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 21ms/step - val_loss: 0.6738\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 19ms/step - val_loss: 0.6738\n",
      "Epoch 59/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "15/15 [==============================] - 0s 20ms/step - val_loss: 0.6738\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 64/100\n",
      "13/15 [=========================>....] - ETA: 0s\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 74/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 79/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 84/100\n",
      "12/15 [=======================>......] - ETA: 0s\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 94/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b88371908>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)]\n",
    "gnn.fit((particles_train, A_tilde), A, epochs=100, batch_size=128, validation_split=0.25, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "particles_test = particles[3000:4000]\n",
    "A_test = make_adjacencies(particles_test)\n",
    "A_tilde_test = normalized_adjacency(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer graph_autoencoder_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z, A_pred = gnn((particles_test, A_tilde_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco = (tf.nn.sigmoid(A_pred) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_test, A_pred), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6930421"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and predict signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the correct signal filename \n",
    "filename = 'signal_file'\n",
    "ff = h5py.File(filename, 'r')\n",
    "particles_signal = np.asarray(ff.get('Particles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691283, 19, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: array([b'Pt', b'Eta', b'Phi', b'Class'], dtype='|S5')\n",
    "particles_signal_test = particles_signal[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "A_signal = make_adjacencies(particles_signal_test)\n",
    "A_tilde_signal = normalized_adjacency(A_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPPUlEQVR4nO3db4wcd33H8fenNg5gaALEQqmdcI4ucusHFUSnAAIh1EJrE5xUFLW2kAqVGyu0qUr7oHXUqhLPQlVVLVJKapU0VUUd3JRSJzEKlD+KVEUhFwhgYwzXYIgjwAZEkFClEPj2wY7J9eJz9m53PXu/vF/SyTO/3dv5+Hbue3PfmZtfqgpJUlt+ru8AkqTxs7hLUoMs7pLUIIu7JDXI4i5JDVrfdwCASy+9tGZmZvqOIUlrysMPP/zdqtp0rsd6Le5JdgG7ZmdnmZ+f7zOKJK05Sb6x3GO9tmWq6u6q2nfxxRf3GUOSmtNrcU+yK8mBJ554os8YktQcj9wlqUFeLSNJDbItI0kNsi0jSQ2yLSNJDbItI0kN6vWPmKrqbuDuubm5G1b7GjP77x1jopU5ecu1vW1bks7HtowkNci2jCQ1yKtlJKlBtmUkqUEWd0lqkD13SWqQPXdJapBtGUlqkMVdkhpkcZekBnlCVZIa5AlVSWqQbRlJapDFXZIaZHGXpAZZ3CWpQRZ3SWqQxV2SGuR17pLUIK9zl6QG2ZaRpAZZ3CWpQRZ3SWqQxV2SGmRxl6QGWdwlqUEWd0lq0ESKe5KNSeaTvHUSry9JOr+hinuS25OcTnJ0yfiOJCeSLCTZv+ihPwMOjTOoJGl4wx653wHsWDyQZB1wK7AT2A7sSbI9yZuBLwOnx5hTkrQC64d5UlXdn2RmyfA1wEJVPQqQ5E7geuBFwEYGBf9/kxypqp8ufc0k+4B9AFdcccWq/wOSpGcaqrgvYzPw2KL1U8Crq+omgCTvAr57rsIOUFUHgAMAc3NzNUIOSdISoxT386qqO57tOUl2AbtmZ2cnFUOSnpNGuVrmceDyRetburGheVdISZqMUYr7Q8BVSbYm2QDsBg6v5AW8n7skTcawl0IeBB4AtiU5lWRvVT0F3ATcBxwHDlXVsZVs3CN3SZqMYa+W2bPM+BHgyFgTSZJG5jR7ktQgp9mTpAZ55C5JDfLIXZIa5C1/JalBFndJapA9d0lqkD13SWqQbRlJapDFXZIaZM9dkhpkz12SGmRbRpIaZHGXpAZZ3CWpQZ5QlaQGeUJVkhpkW0aSGmRxl6QGWdwlqUEWd0lqkMVdkhrkpZCS1CAvhZSkBtmWkaQGre87wFo2s//eXrZ78pZre9mupLXDI3dJapDFXZIaZHGXpAZZ3CWpQRZ3SWqQxV2SGjT24p7kl5LcluSuJO8e9+tLkp7dUMU9ye1JTic5umR8R5ITSRaS7AeoquNVdSPwW8Drxh9ZkvRshj1yvwPYsXggyTrgVmAnsB3Yk2R799h1wL3AkbEllSQNbajiXlX3A99fMnwNsFBVj1bVk8CdwPXd8w9X1U7gHcu9ZpJ9SeaTzJ85c2Z16SVJ5zTK7Qc2A48tWj8FvDrJG4G3ARdxniP3qjoAHACYm5urEXJIkpYY+71lquozwGeGeW6SXcCu2dnZcceQpOe0Ua6WeRy4fNH6lm5saN7yV5ImY5Ti/hBwVZKtSTYAu4HDK3kBJ+uQpMkY9lLIg8ADwLYkp5LsraqngJuA+4DjwKGqOraSjXvkLkmTMVTPvar2LDN+hBEud7TnLkmT4TR7ktQg7y0jSQ3qtbh7QlWSJsO2jCQ1yLaMJDXItowkNci2jCQ1yLaMJDXI4i5JDbLnLkkNsucuSQ2yLSNJDbK4S1KDLO6S1CCLuyQ1aOxzqK6E93NfnZn99/a27ZO3XNvbtiUNz6tlJKlBtmUkqUEWd0lqkMVdkhpkcZekBlncJalB3jhMkhrkpZCS1CDbMpLUIIu7JDXI4i5JDbK4S1KDLO6S1CCLuyQ1yOIuSQ2ayP3ck/wGcC3w88AHq+rjk9iOJOnchj5yT3J7ktNJji4Z35HkRJKFJPsBquqjVXUDcCPw2+ONLEl6Nitpy9wB7Fg8kGQdcCuwE9gO7EmyfdFT/qJ7XJJ0AQ1d3KvqfuD7S4avARaq6tGqehK4E7g+A+8DPlZVnzvX6yXZl2Q+yfyZM2dWm1+SdA6jnlDdDDy2aP1UN/aHwJuAtye58VyfWFUHqmququY2bdo0YgxJ0mITOaFaVe8H3v9sz3OC7LWnr8m5nZhbWplRj9wfBy5ftL6lGxuKd4WUpMkYtbg/BFyVZGuSDcBu4PDosSRJo1jJpZAHgQeAbUlOJdlbVU8BNwH3AceBQ1V1bAWv6WQdkjQBQ/fcq2rPMuNHgCOr2XhV3Q3cPTc3d8NqPl+SdG5OsydJDXKaPUlqkDcOk6QG2ZaRpAbZlpGkBtmWkaQG2ZaRpAbZlpGkBtmWkaQGWdwlqUH23CWpQfbcJalBtmUkqUEWd0lqkMVdkhrkCVVJapAnVCWpQbZlJKlBFndJatDQc6hKfZrZf28v2z15y7W9bFcalUfuktQgi7skNcjiLkkN8jp3SWqQ17lLUoNsy0hSgyzuktQgi7skNcjiLkkNsrhLUoMs7pLUIIu7JDVo7MU9yZVJPpjkrnG/tiRpOEMV9yS3Jzmd5OiS8R1JTiRZSLIfoKoeraq9kwgrSRrOsEfudwA7Fg8kWQfcCuwEtgN7kmwfazpJ0qoMVdyr6n7g+0uGrwEWuiP1J4E7geuH3XCSfUnmk8yfOXNm6MCSpGc3Ss99M/DYovVTwOYkL0tyG/CqJDcv98lVdaCq5qpqbtOmTSPEkCQtNfaZmKrqe8CNwzw3yS5g1+zs7LhjSGPR1wxQ4CxQGs0oR+6PA5cvWt/SjQ3Nu0JK0mSMUtwfAq5KsjXJBmA3cHglL+D93CVpMoa9FPIg8ACwLcmpJHur6ingJuA+4DhwqKqOrWTjHrlL0mQM1XOvqj3LjB8Bjow1kSRpZGM/oboSnlCVpo8nkdvgNHuS1CAnyJakBnnkLkkN8pa/ktQgi7skNcieuyQ1yJ67JDXItowkNcjiLkkN8i9UJT3ntfhXufbcJalBtmUkqUEWd0lqkMVdkhrkHzFJUoM8oSpJDbItI0kNsrhLUoMs7pLUIIu7JDXI4i5JDfJSSElqkJdCSlKDbMtIUoMs7pLUIIu7JDXI4i5JDbK4S1KDLO6S1CCLuyQ1yOIuSQ1aP+4XTLIR+HvgSeAzVfWhcW9DknR+Qx25J7k9yekkR5eM70hyIslCkv3d8NuAu6rqBuC6MeeVJA1h2LbMHcCOxQNJ1gG3AjuB7cCeJNuBLcBj3dN+Mp6YkqSVGKotU1X3J5lZMnwNsFBVjwIkuRO4HjjFoMA/wnl+eCTZB+wDuOKKK1aaW2rezP57+45wwT0X/8+TMsoJ1c08fYQOg6K+GfgI8JtJPgDcvdwnV9WBqpqrqrlNmzaNEEOStNTYT6hW1Y+A3x3muUl2AbtmZ2fHHUOSntNGOXJ/HLh80fqWbmxo3vJXkiZjlOL+EHBVkq1JNgC7gcMreQEn65CkyRj2UsiDwAPAtiSnkuytqqeAm4D7gOPAoao6tpKNe+QuSZMx7NUye5YZPwIcWe3G7blL0mQ4zZ4kNch7y0hSg3ot7p5QlaTJSFX1nYEkZ4BvrPLTLwW+O8Y44zSt2aY1F5htNaY1F0xvtmnNBSvL9oqqOudfgU5FcR9Fkvmqmus7x7lMa7ZpzQVmW41pzQXTm21ac8H4stlzl6QGWdwlqUEtFPcDfQc4j2nNNq25wGyrMa25YHqzTWsuGFO2Nd9zlyQ9UwtH7pKkJSzuktSgNV3cl5nD9UJu/xlzyyZ5aZJPJPla9+9LuvEkeX+X9YtJrp5grsuTfDrJl5McS/JH05AtyfOTfDbJF7pc7+3GtyZ5sNv+h7u7jJLkom59oXt8ZhK5lmRcl+TzSe6ZpmxJTib5UpJHksx3Y9Owr12S5K4kX0lyPMlrpyTXtu5rdfbjh0neMyXZ/rjb/48mOdh9X4x/P6uqNfkBrAP+B7gS2AB8Adh+gTO8AbgaOLpo7K+A/d3yfuB93fJbgI8BAV4DPDjBXJcBV3fLLwa+ymCe216zda//om75ecCD3fYOAbu78duAd3fLvw/c1i3vBj58Ad7TPwH+FbinW5+KbMBJ4NIlY9Owr/0z8Hvd8gbgkmnItSTjOuDbwCv6zsZgtrqvAy9YtH+9axL72cS/sBN8w14L3Ldo/Wbg5h5yzPD/i/sJ4LJu+TLgRLf8D8Cecz3vAmT8T+DN05QNeCHwOeDVDP4ab/3S95XB7aRf2y2v756XCWbaAnwS+BXgnu4bfVqyneSZxb3X9xO4uCtUmaZc58j5a8B/T0M2np6e9KXdfnMP8OuT2M/WcltmuTlc+/byqvpWt/xt4OXdci95u1/jXsXgKLn3bF3b4xHgNPAJBr99/aAG8wMs3fbPcnWPPwG8bBK5On8L/Cnw0279ZVOUrYCPJ3k4g8nlof/3cytwBvinrpX1j0k2TkGupXYDB7vlXrNV1ePAXwPfBL7FYL95mAnsZ2u5uE+9Gvy47e1a0yQvAv4deE9V/XDxY31lq6qfVNUrGRwlXwP84oXOcC5J3gqcrqqH+86yjNdX1dXATuAPkrxh8YM9vZ/rGbQlP1BVrwJ+xKDV0Xeun+l619cB/7b0sT6ydT3+6xn8YPwFYCOwYxLbWsvFfeQ5XCfkO0kuA+j+Pd2NX9C8SZ7HoLB/qKo+Mk3ZAKrqB8CnGfwKekmSsxPHLN72z3J1j18MfG9CkV4HXJfkJHAng9bM301JtrNHfFTVaeA/GPxg7Pv9PAWcqqoHu/W7GBT7vnMtthP4XFV9p1vvO9ubgK9X1Zmq+jHwEQb73tj3s7Vc3Eeew3VCDgPv7JbfyaDffXb8d7qz8q8Bnlj06+FYJQnwQeB4Vf3NtGRLsinJJd3yCxicBzjOoMi/fZlcZ/O+HfhUd7Q1dlV1c1VtqaoZBvvSp6rqHdOQLcnGJC8+u8ygh3yUnt/Pqvo28FiSbd3QrwJf7jvXEnt4uiVzNkOf2b4JvCbJC7vv07Nfs/HvZ5M+mTHJDwZnuL/KoG/75z1s/yCDvtmPGRzF7GXQD/sk8DXgv4CXds8NcGuX9UvA3ARzvZ7Br5tfBB7pPt7Sdzbgl4HPd7mOAn/ZjV8JfBZYYPDr80Xd+PO79YXu8Ssv0Pv6Rp6+Wqb3bF2GL3Qfx87u632/n922XgnMd+/pR4GXTEOubnsbGRzlXrxorPdswHuBr3TfA/8CXDSJ/czbD0hSg9ZyW0aStAyLuyQ1yOIuSQ2yuEtSgyzuktQgi7skNcjiLkkN+j9JZDl3adTEnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_signal_test[:,:,0].flatten())\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_signal_test = normalize_features(particles_signal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'normalized pt')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_signal_test[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('normalized pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_signal, A_pred_signal = gnn((particles_signal_test, A_tilde_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco_signal = (tf.nn.sigmoid(A_pred_signal) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_signal, A_pred_signal), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69312567"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model: Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphVariationalAutoencoder(nodes_n=nodes_n, feat_sz=feat_sz, activation=tf.nn.tanh)\n",
    "gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.8048 - loss_reco: 0.8033 - loss_latent: 0.0015 - val_loss: 0.8070 - val_loss_reco: 0.8067 - val_loss_latent: 2.8398e-04\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.8015 - loss_reco: 0.8014 - loss_latent: 1.1290e-04 - val_loss: 0.7963 - val_loss_reco: 0.7962 - val_loss_latent: 2.3983e-05\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.8014 - loss_reco: 0.8014 - loss_latent: 5.3270e-05 - val_loss: 0.8049 - val_loss_reco: 0.8048 - val_loss_latent: 9.3976e-05\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7993 - loss_reco: 0.7989 - loss_latent: 3.4072e-04 - val_loss: 0.7990 - val_loss_reco: 0.7984 - val_loss_latent: 5.8588e-04\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7996 - loss_reco: 0.7982 - loss_latent: 0.0014 - val_loss: 0.8017 - val_loss_reco: 0.7998 - val_loss_latent: 0.0019\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7994 - loss_reco: 0.7971 - loss_latent: 0.0024 - val_loss: 0.7984 - val_loss_reco: 0.7962 - val_loss_latent: 0.0022\n",
      "Epoch 7/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7959 - loss_reco: 0.7936 - loss_latent: 0.0023\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7971 - loss_reco: 0.7949 - loss_latent: 0.0022 - val_loss: 0.7968 - val_loss_reco: 0.7950 - val_loss_latent: 0.0018\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7973 - loss_reco: 0.7953 - loss_latent: 0.0020 - val_loss: 0.8113 - val_loss_reco: 0.8095 - val_loss_latent: 0.0018\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7975 - loss_reco: 0.7955 - loss_latent: 0.0020 - val_loss: 0.7985 - val_loss_reco: 0.7966 - val_loss_latent: 0.0018\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7974 - loss_reco: 0.7954 - loss_latent: 0.0021 - val_loss: 0.8139 - val_loss_reco: 0.8120 - val_loss_latent: 0.0019\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7990 - loss_reco: 0.7969 - loss_latent: 0.0021 - val_loss: 0.7984 - val_loss_reco: 0.7965 - val_loss_latent: 0.0020\n",
      "Epoch 12/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7964 - loss_reco: 0.7943 - loss_latent: 0.0021\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7959 - loss_reco: 0.7938 - loss_latent: 0.0022 - val_loss: 0.7989 - val_loss_reco: 0.7969 - val_loss_latent: 0.0020\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7972 - loss_reco: 0.7951 - loss_latent: 0.0021 - val_loss: 0.7918 - val_loss_reco: 0.7899 - val_loss_latent: 0.0019\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7996 - loss_reco: 0.7975 - loss_latent: 0.0021 - val_loss: 0.7970 - val_loss_reco: 0.7950 - val_loss_latent: 0.0020\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7977 - loss_reco: 0.7956 - loss_latent: 0.0022 - val_loss: 0.8035 - val_loss_reco: 0.8016 - val_loss_latent: 0.0020\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7978 - loss_reco: 0.7957 - loss_latent: 0.0021 - val_loss: 0.7939 - val_loss_reco: 0.7919 - val_loss_latent: 0.0019\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.8004 - loss_reco: 0.7983 - loss_latent: 0.0021 - val_loss: 0.8134 - val_loss_reco: 0.8115 - val_loss_latent: 0.0020\n",
      "Epoch 18/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7958 - loss_reco: 0.7937 - loss_latent: 0.0021\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7962 - loss_reco: 0.7941 - loss_latent: 0.0021 - val_loss: 0.8057 - val_loss_reco: 0.8038 - val_loss_latent: 0.0019\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.8000 - loss_reco: 0.7980 - loss_latent: 0.0021 - val_loss: 0.8045 - val_loss_reco: 0.8026 - val_loss_latent: 0.0019\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7994 - loss_reco: 0.7973 - loss_latent: 0.0021 - val_loss: 0.8045 - val_loss_reco: 0.8025 - val_loss_latent: 0.0019\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7964 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.8047 - val_loss_reco: 0.8028 - val_loss_latent: 0.0019\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7983 - loss_reco: 0.7962 - loss_latent: 0.0021 - val_loss: 0.8096 - val_loss_reco: 0.8076 - val_loss_latent: 0.0019\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7960 - loss_reco: 0.7939 - loss_latent: 0.0021 - val_loss: 0.7914 - val_loss_reco: 0.7894 - val_loss_latent: 0.0020\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.7966 - loss_reco: 0.7945 - loss_latent: 0.0021 - val_loss: 0.8045 - val_loss_reco: 0.8026 - val_loss_latent: 0.0019\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7958 - loss_reco: 0.7937 - loss_latent: 0.0021 - val_loss: 0.7969 - val_loss_reco: 0.7950 - val_loss_latent: 0.0019\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.8014 - loss_reco: 0.7993 - loss_latent: 0.0021 - val_loss: 0.8001 - val_loss_reco: 0.7981 - val_loss_latent: 0.0020\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.8004 - loss_reco: 0.7983 - loss_latent: 0.0021 - val_loss: 0.8049 - val_loss_reco: 0.8029 - val_loss_latent: 0.0020\n",
      "Epoch 28/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7977 - loss_reco: 0.7956 - loss_latent: 0.0021\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7969 - loss_reco: 0.7948 - loss_latent: 0.0021 - val_loss: 0.8003 - val_loss_reco: 0.7983 - val_loss_latent: 0.0020\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7983 - loss_reco: 0.7962 - loss_latent: 0.0021 - val_loss: 0.8106 - val_loss_reco: 0.8086 - val_loss_latent: 0.0020\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7994 - loss_reco: 0.7972 - loss_latent: 0.0022 - val_loss: 0.7958 - val_loss_reco: 0.7938 - val_loss_latent: 0.0020\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7957 - loss_reco: 0.7935 - loss_latent: 0.0022 - val_loss: 0.8060 - val_loss_reco: 0.8040 - val_loss_latent: 0.0020\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7976 - loss_reco: 0.7954 - loss_latent: 0.0022 - val_loss: 0.7968 - val_loss_reco: 0.7948 - val_loss_latent: 0.0020\n",
      "Epoch 33/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7970 - loss_reco: 0.7949 - loss_latent: 0.0021\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7965 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.7974 - val_loss_reco: 0.7954 - val_loss_latent: 0.0020\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7993 - loss_reco: 0.7972 - loss_latent: 0.0021 - val_loss: 0.8033 - val_loss_reco: 0.8013 - val_loss_latent: 0.0020\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.8002 - loss_reco: 0.7981 - loss_latent: 0.0021 - val_loss: 0.8138 - val_loss_reco: 0.8118 - val_loss_latent: 0.0020\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.7955 - loss_reco: 0.7933 - loss_latent: 0.0021 - val_loss: 0.7976 - val_loss_reco: 0.7956 - val_loss_latent: 0.0020\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7964 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.8023 - val_loss_reco: 0.8004 - val_loss_latent: 0.0020\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7982 - loss_reco: 0.7961 - loss_latent: 0.0021\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7984 - loss_reco: 0.7963 - loss_latent: 0.0021 - val_loss: 0.8007 - val_loss_reco: 0.7987 - val_loss_latent: 0.0020\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.8013 - loss_reco: 0.7992 - loss_latent: 0.0021 - val_loss: 0.8024 - val_loss_reco: 0.8004 - val_loss_latent: 0.0020\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7975 - loss_reco: 0.7954 - loss_latent: 0.0021 - val_loss: 0.7990 - val_loss_reco: 0.7971 - val_loss_latent: 0.0020\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7990 - loss_reco: 0.7969 - loss_latent: 0.0021 - val_loss: 0.8114 - val_loss_reco: 0.8095 - val_loss_latent: 0.0020\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7965 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.7977 - val_loss_reco: 0.7957 - val_loss_latent: 0.0020\n",
      "Epoch 43/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7960 - loss_reco: 0.7940 - loss_latent: 0.0021\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7946 - loss_reco: 0.7924 - loss_latent: 0.0021 - val_loss: 0.7924 - val_loss_reco: 0.7904 - val_loss_latent: 0.0020\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7982 - loss_reco: 0.7961 - loss_latent: 0.0021 - val_loss: 0.8081 - val_loss_reco: 0.8061 - val_loss_latent: 0.0020\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7954 - loss_reco: 0.7933 - loss_latent: 0.0021 - val_loss: 0.7973 - val_loss_reco: 0.7953 - val_loss_latent: 0.0020\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7956 - loss_reco: 0.7935 - loss_latent: 0.0021 - val_loss: 0.8050 - val_loss_reco: 0.8030 - val_loss_latent: 0.0020\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7971 - loss_reco: 0.7949 - loss_latent: 0.0021 - val_loss: 0.8054 - val_loss_reco: 0.8034 - val_loss_latent: 0.0020\n",
      "Epoch 48/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7986 - loss_reco: 0.7965 - loss_latent: 0.0021\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7980 - loss_reco: 0.7959 - loss_latent: 0.0021 - val_loss: 0.8078 - val_loss_reco: 0.8058 - val_loss_latent: 0.0020\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7969 - loss_reco: 0.7947 - loss_latent: 0.0021 - val_loss: 0.8055 - val_loss_reco: 0.8035 - val_loss_latent: 0.0020\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7972 - loss_reco: 0.7951 - loss_latent: 0.0021 - val_loss: 0.8004 - val_loss_reco: 0.7985 - val_loss_latent: 0.0020\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7968 - loss_reco: 0.7947 - loss_latent: 0.0021 - val_loss: 0.8088 - val_loss_reco: 0.8068 - val_loss_latent: 0.0020\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7978 - loss_reco: 0.7956 - loss_latent: 0.0021 - val_loss: 0.8141 - val_loss_reco: 0.8121 - val_loss_latent: 0.0020\n",
      "Epoch 53/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7985 - loss_reco: 0.7964 - loss_latent: 0.0021\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7968 - loss_reco: 0.7947 - loss_latent: 0.0021 - val_loss: 0.8035 - val_loss_reco: 0.8015 - val_loss_latent: 0.0020\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7987 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.7909 - val_loss_reco: 0.7889 - val_loss_latent: 0.0020\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.8002 - loss_reco: 0.7980 - loss_latent: 0.0021 - val_loss: 0.8046 - val_loss_reco: 0.8026 - val_loss_latent: 0.0020\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.8000 - loss_reco: 0.7979 - loss_latent: 0.0021 - val_loss: 0.7963 - val_loss_reco: 0.7943 - val_loss_latent: 0.0020\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7973 - loss_reco: 0.7951 - loss_latent: 0.0021 - val_loss: 0.8021 - val_loss_reco: 0.8002 - val_loss_latent: 0.0020\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7988 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.7948 - val_loss_reco: 0.7928 - val_loss_latent: 0.0020\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7972 - loss_reco: 0.7951 - loss_latent: 0.0021\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7975 - loss_reco: 0.7954 - loss_latent: 0.0021 - val_loss: 0.7984 - val_loss_reco: 0.7965 - val_loss_latent: 0.0020\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7950 - loss_reco: 0.7929 - loss_latent: 0.0021 - val_loss: 0.8013 - val_loss_reco: 0.7994 - val_loss_latent: 0.0020\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7997 - loss_reco: 0.7976 - loss_latent: 0.0021 - val_loss: 0.8029 - val_loss_reco: 0.8009 - val_loss_latent: 0.0020\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7962 - loss_reco: 0.7941 - loss_latent: 0.0021 - val_loss: 0.7923 - val_loss_reco: 0.7903 - val_loss_latent: 0.0020\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7987 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.8130 - val_loss_reco: 0.8111 - val_loss_latent: 0.0020\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7996 - loss_reco: 0.7974 - loss_latent: 0.0021\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.8000 - loss_reco: 0.7978 - loss_latent: 0.0021 - val_loss: 0.7996 - val_loss_reco: 0.7977 - val_loss_latent: 0.0020\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7956 - loss_reco: 0.7935 - loss_latent: 0.0021 - val_loss: 0.8025 - val_loss_reco: 0.8005 - val_loss_latent: 0.0020\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7981 - loss_reco: 0.7959 - loss_latent: 0.0021 - val_loss: 0.8004 - val_loss_reco: 0.7984 - val_loss_latent: 0.0020\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7991 - loss_reco: 0.7970 - loss_latent: 0.0021 - val_loss: 0.7962 - val_loss_reco: 0.7942 - val_loss_latent: 0.0020\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7984 - loss_reco: 0.7963 - loss_latent: 0.0021 - val_loss: 0.7971 - val_loss_reco: 0.7952 - val_loss_latent: 0.0020\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7981 - loss_reco: 0.7960 - loss_latent: 0.0021\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7982 - loss_reco: 0.7961 - loss_latent: 0.0021 - val_loss: 0.7991 - val_loss_reco: 0.7971 - val_loss_latent: 0.0020\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7997 - loss_reco: 0.7976 - loss_latent: 0.0021 - val_loss: 0.7890 - val_loss_reco: 0.7870 - val_loss_latent: 0.0020\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7977 - loss_reco: 0.7956 - loss_latent: 0.0021 - val_loss: 0.8111 - val_loss_reco: 0.8091 - val_loss_latent: 0.0020\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7972 - loss_reco: 0.7951 - loss_latent: 0.0021 - val_loss: 0.8048 - val_loss_reco: 0.8028 - val_loss_latent: 0.0020\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7979 - loss_reco: 0.7958 - loss_latent: 0.0021 - val_loss: 0.8086 - val_loss_reco: 0.8066 - val_loss_latent: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7960 - loss_reco: 0.7939 - loss_latent: 0.0021 - val_loss: 0.7941 - val_loss_reco: 0.7921 - val_loss_latent: 0.0020\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7973 - loss_reco: 0.7952 - loss_latent: 0.0021\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7981 - loss_reco: 0.7960 - loss_latent: 0.0021 - val_loss: 0.7898 - val_loss_reco: 0.7878 - val_loss_latent: 0.0020\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7999 - loss_reco: 0.7977 - loss_latent: 0.0021 - val_loss: 0.7975 - val_loss_reco: 0.7955 - val_loss_latent: 0.0020\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7953 - loss_reco: 0.7931 - loss_latent: 0.0021 - val_loss: 0.7991 - val_loss_reco: 0.7971 - val_loss_latent: 0.0020\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7987 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.8009 - val_loss_reco: 0.7989 - val_loss_latent: 0.0020\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7979 - loss_reco: 0.7958 - loss_latent: 0.0021 - val_loss: 0.8018 - val_loss_reco: 0.7998 - val_loss_latent: 0.0020\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.8002 - loss_reco: 0.7981 - loss_latent: 0.0021 - val_loss: 0.7878 - val_loss_reco: 0.7858 - val_loss_latent: 0.0020\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7961 - loss_reco: 0.7939 - loss_latent: 0.0021 - val_loss: 0.8072 - val_loss_reco: 0.8052 - val_loss_latent: 0.0020\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7975 - loss_reco: 0.7954 - loss_latent: 0.0021 - val_loss: 0.7990 - val_loss_reco: 0.7970 - val_loss_latent: 0.0020\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7988 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.8005 - val_loss_reco: 0.7986 - val_loss_latent: 0.0020\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7981 - loss_reco: 0.7960 - loss_latent: 0.0021 - val_loss: 0.8023 - val_loss_reco: 0.8003 - val_loss_latent: 0.0020\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.7986 - loss_reco: 0.7965 - loss_latent: 0.0021\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7996 - loss_reco: 0.7975 - loss_latent: 0.0021 - val_loss: 0.8090 - val_loss_reco: 0.8070 - val_loss_latent: 0.0020\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7987 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.7996 - val_loss_reco: 0.7976 - val_loss_latent: 0.0020\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7978 - loss_reco: 0.7957 - loss_latent: 0.0021 - val_loss: 0.7991 - val_loss_reco: 0.7971 - val_loss_latent: 0.0020\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7971 - loss_reco: 0.7950 - loss_latent: 0.0021 - val_loss: 0.7918 - val_loss_reco: 0.7898 - val_loss_latent: 0.0020\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7956 - loss_reco: 0.7935 - loss_latent: 0.0021 - val_loss: 0.8016 - val_loss_reco: 0.7996 - val_loss_latent: 0.0020\n",
      "Epoch 90/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7992 - loss_reco: 0.7971 - loss_latent: 0.0021\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.8008 - loss_reco: 0.7987 - loss_latent: 0.0021 - val_loss: 0.8087 - val_loss_reco: 0.8067 - val_loss_latent: 0.0020\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7968 - loss_reco: 0.7947 - loss_latent: 0.0021 - val_loss: 0.8089 - val_loss_reco: 0.8069 - val_loss_latent: 0.0020\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7990 - loss_reco: 0.7969 - loss_latent: 0.0021 - val_loss: 0.8044 - val_loss_reco: 0.8024 - val_loss_latent: 0.0020\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7977 - loss_reco: 0.7955 - loss_latent: 0.0021 - val_loss: 0.8007 - val_loss_reco: 0.7987 - val_loss_latent: 0.0020\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7964 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.7928 - val_loss_reco: 0.7908 - val_loss_latent: 0.0020\n",
      "Epoch 95/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7995 - loss_reco: 0.7973 - loss_latent: 0.0021\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7973 - loss_reco: 0.7952 - loss_latent: 0.0021 - val_loss: 0.7957 - val_loss_reco: 0.7937 - val_loss_latent: 0.0020\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.8001 - loss_reco: 0.7980 - loss_latent: 0.0021 - val_loss: 0.8052 - val_loss_reco: 0.8032 - val_loss_latent: 0.0020\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7959 - loss_reco: 0.7937 - loss_latent: 0.0021 - val_loss: 0.8113 - val_loss_reco: 0.8093 - val_loss_latent: 0.0020\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7990 - loss_reco: 0.7969 - loss_latent: 0.0021 - val_loss: 0.7957 - val_loss_reco: 0.7937 - val_loss_latent: 0.0020\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.8006 - loss_reco: 0.7984 - loss_latent: 0.0021 - val_loss: 0.8102 - val_loss_reco: 0.8082 - val_loss_latent: 0.0020\n",
      "Epoch 100/100\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 0.7948 - loss_reco: 0.7927 - loss_latent: 0.0021\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7938 - loss_reco: 0.7916 - loss_latent: 0.0021 - val_loss: 0.8074 - val_loss_reco: 0.8054 - val_loss_latent: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b76843710>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)]\n",
    "gnn.fit((particles_train, A_tilde), A, epochs=100, batch_size=batch_size, validation_split=0.25, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer graph_variational_autoencoder_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z, z_mean, z_log_var, A_pred = gnn((particles_test, A_tilde_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco = (tf.nn.sigmoid(A_pred) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background_reco = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_test, A_pred), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background_latent = kl_loss(z_mean, z_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112967"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036986305"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_signal, z_mean_signal, z_log_var_signal, A_pred_signal = gnn((particles_signal_test, A_tilde_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco_signal = (tf.nn.sigmoid(A_pred_signal) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal_reco = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_signal, A_pred_signal), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal_latent = kl_loss(z_mean_signal, z_log_var_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8038542"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005094626"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal_latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
